{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ff7b1d",
   "metadata": {},
   "source": [
    "# Fine-tuning Mistral-7B for Instruction Generation\n",
    "\n",
    "## Overview\n",
    "This Jupyter notebook demonstrates the process of fine-tuning the Mistral-7B language model for instruction generation using Parameter-Efficient Fine-Tuning (PEFT) with Low-Rank Adaptation (LoRA). The goal is to adapt the model to generate instructions based on given inputs and responses, essentially reversing the typical instruction-following behavior of large language models.\n",
    "\n",
    "## Purpose\n",
    "- Showcase the fine-tuning process for large language models\n",
    "- Demonstrate the use of LoRA for efficient adaptation of pre-trained models\n",
    "- Provide a practical example of preparing data, configuring models, and training for a specific NLP task\n",
    "- Include a section on deploying the fine-tuned model using FastAPI and testing the model's performance using the checkpoints created during the fine-tuning process\n",
    "\n",
    "## Key Components\n",
    "1. Data preparation using the mosaicml/instruct-v3 dataset\n",
    "2. Model loading and configuration with 4-bit quantization\n",
    "3. LoRA setup for parameter-efficient fine-tuning\n",
    "4. Training process using the SFTTrainer from the TRL library\n",
    "5. Model deployment using FastAPI\n",
    "6. Testing the fine-tuned model with sample prompts\n",
    "\n",
    "## How to Use This Notebook\n",
    "1. **Environment Setup**: Ensure you have a GPU-enabled environment with Python and Jupyter installed.\n",
    "2. **Dependencies**: Run the first cell to install required libraries.\n",
    "3. **Data Preparation**: Follow the cells that load and preprocess the dataset.\n",
    "4. **Model Configuration**: Execute cells that load and configure the Mistral-7B model.\n",
    "5. **Training**: Run the training cell to fine-tune the model.\n",
    "6. **Evaluation**: Use the provided functions to test the model's performance after training.\n",
    "7. **Deployment**: Explore the section that deploys the fine-tuned model as a FastAPI web service.\n",
    "8. **Testing**: Test the deployed model with sample prompts and inspect the generated responses.\n",
    "\n",
    "## Notes\n",
    "- This notebook uses a subset of the full dataset for quicker experimentation. Adjust dataset size as needed.\n",
    "- The training process is resource-intensive. Ensure you have adequate GPU memory available.\n",
    "- Experiment with different LoRA configurations and training parameters to optimize results.\n",
    "- The deployment and testing sections leverage the checkpoints created during the fine-tuning process.\n",
    "\n",
    "By following this notebook, you'll gain hands-on experience in fine-tuning large language models for specific tasks using state-of-the-art techniques in natural language processing, as well as deploying the fine-tuned model as a web service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40505600",
   "metadata": {},
   "source": [
    "## Installing Required Libraries\n",
    "\n",
    "**What it's doing:**\n",
    "Installing necessary Python libraries for the project.\n",
    "\n",
    "**Why:**\n",
    "These libraries are essential for working with transformers, fine-tuning models, handling datasets, and optimizing performance. Installing them ensures we have all the tools needed for our task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2c5907-8fd0-47c2-bbf0-12a4d26471bb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.2.0+cu121 requires torch==2.2.0, but you have torch 2.3.1 which is incompatible.\n",
      "torchvision 0.17.0+cu121 requires torch==2.2.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers trl accelerate torch bitsandbytes peft datasets -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa936223",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "**What it's doing:**\n",
    "Loading the \"mosaicml/instruct-v3\" dataset.\n",
    "\n",
    "**Why:**\n",
    "This dataset contains instruction-response pairs, which are crucial for our task of fine-tuning a model to generate instructions. It provides the training data we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c5d2739-a164-4393-8e49-7abcaa434eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "instruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9c35c",
   "metadata": {},
   "source": [
    "## Examining the Dataset\n",
    "\n",
    "**What it's doing:**\n",
    "Displaying the structure of the loaded dataset.\n",
    "\n",
    "**Why:**\n",
    "This helps us understand the composition of our dataset, including the number of examples and the available features. It's an important step for data exploration and verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69a89a17-9001-452a-895a-5c3e5203ee13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 56167\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 6807\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_tune_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a568007",
   "metadata": {},
   "source": [
    "## Filtering the Dataset\n",
    "\n",
    "**What it's doing:**\n",
    "Filtering the dataset to only include examples from the \"dolly_hhrlhf\" source.\n",
    "\n",
    "**Why:**\n",
    "By focusing on a specific subset of the data, we can potentially improve the quality and consistency of our fine-tuning results. This step helps in data curation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c320919-dfc2-47c6-9d51-770640391775",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 34333\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 4771\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\")\n",
    "instruct_tune_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac3e9f",
   "metadata": {},
   "source": [
    "## Reducing Dataset Size\n",
    "\n",
    "**What it's doing:**\n",
    "Limiting the dataset to 5,000 training examples and 200 test examples.\n",
    "\n",
    "**Why:**\n",
    "This reduction in dataset size allows for faster experimentation and requires less computational resources. It's a common practice when initially developing and testing a model fine-tuning pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd23da7-e288-4e0e-b1f2-6c80e46d9080",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_tune_dataset[\"train\"] = instruct_tune_dataset[\"train\"].select(range(5_000))\n",
    "instruct_tune_dataset[\"test\"] = instruct_tune_dataset[\"test\"].select(range(200))\n",
    "instruct_tune_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1915b3",
   "metadata": {},
   "source": [
    "## Defining the Prompt Template\n",
    "\n",
    "**What it's doing:**\n",
    "Creating a template for formatting our training data.\n",
    "\n",
    "**Why:**\n",
    "This template structures our input data consistently, telling the model how to interpret the input and what kind of output we expect. It's crucial for instruction-tuning tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a8225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>### Instruction:\n",
    "Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{response}</s>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9029e",
   "metadata": {},
   "source": [
    "## Creating the Prompt Function\n",
    "\n",
    "**What it's doing:**\n",
    "Defining a function to format each sample from our dataset according to the prompt template.\n",
    "\n",
    "**Why:**\n",
    "This function prepares our data for training, ensuring each example is formatted consistently and correctly for our specific task of instruction generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30068c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(sample):\n",
    "    input_text = sample[\"response\"]  # The 'response' from the dataset becomes the 'input' for our new task\n",
    "    response_text = sample[\"prompt\"].replace(\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction\\n\", \"\").strip()\n",
    "    \n",
    "    full_prompt = prompt_template.format(input=input_text, response=response_text)\n",
    "    \n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c279e",
   "metadata": {},
   "source": [
    "## Testing the Prompt Function\n",
    "\n",
    "**What it's doing:**\n",
    "Applying the prompt function to a sample from the dataset.\n",
    "\n",
    "**Why:**\n",
    "This test ensures our prompt function is working correctly before we use it in training. It's a crucial verification step in our data preparation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7a552d-4e55-4043-8e24-ae09ea0c753d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.\\n\\n### Input:\\nThere are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\\n\\n### Response:\\nWhat are different types of grass?\\n\\n### Response</s>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt(instruct_tune_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e2d84f",
   "metadata": {},
   "source": [
    "## Loading the Pre-trained Model and Tokenizer\n",
    "\n",
    "**What it's doing:**\n",
    "Loading the Mistral-7B model and its tokenizer, with 4-bit quantization.\n",
    "\n",
    "**Why:**\n",
    "This step prepares our base model for fine-tuning. The 4-bit quantization allows us to work with this large model on more modest hardware by reducing its memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad67bb92-1be9-466d-b095-e9504920db3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c10b7a0bc64763b4da7790999d93d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    device_map='auto',\n",
    "    quantization_config=nf4_config,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c70e5e",
   "metadata": {},
   "source": [
    "## Defining the Generation Function\n",
    "\n",
    "**What it's doing:**\n",
    "Creating a function to generate responses using our model.\n",
    "\n",
    "**Why:**\n",
    "This function allows us to test our model's outputs at various stages of fine-tuning, helping us assess its performance and progress.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "903b2df4-3a3a-418e-8015-7d73f16d049f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt, model):\n",
    "  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "  return decoded_output[0].replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e1bfb6",
   "metadata": {},
   "source": [
    "## Testing the Generation Function\n",
    "\n",
    "**What it's doing:**\n",
    "Generating a response with our base model before fine-tuning.\n",
    "\n",
    "**Why:**\n",
    "This provides a baseline to compare against after fine-tuning, helping us understand how much the model's performance improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53737251-3fd6-401f-a6e3-ee20fa47606e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> \\nTo become a healthcare professional, such as a medical doctor, you must first obtain a four-year undergraduate degree and a four-year doctorate degree. After completing your education, you must complete a residency program. Once you have successfully completed these steps, you will need to become licensed before establishing a practice.</s>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.\\n\\n### Input:\\nI think it depends a little on the individual, but there are a number of steps you’ll need to take.  First, you’ll need to get a college education.  This might include a four-year undergraduate degree and a four-year doctorate program.  You’ll also need to complete a residency program.  Once you have your education, you’ll need to be licensed.  And finally, you’ll need to establish a practice.\\n\\n### Response:\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46499063",
   "metadata": {},
   "source": [
    "## Configuring LoRA for Fine-tuning\n",
    "\n",
    "**What it's doing:**\n",
    "Setting up the Low-Rank Adaptation (LoRA) configuration for fine-tuning.\n",
    "\n",
    "**Why:**\n",
    "LoRA allows us to fine-tune the model efficiently by adding a small number of trainable parameters. This configuration defines how LoRA will be applied to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c53c0dc8-4447-4a8f-b3fc-b5b39d49740d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c045952",
   "metadata": {},
   "source": [
    "## Preparing the Model for LoRA Fine-tuning\n",
    "\n",
    "**What it's doing:**\n",
    "Applying the LoRA configuration to our model.\n",
    "\n",
    "**Why:**\n",
    "This step prepares our model for efficient fine-tuning, setting up the additional LoRA parameters while keeping most of the original model frozen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1c8904d-7e60-43ee-8495-f9abf3ba40de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='mistralai/Mistral-7B-Instruct-v0.1', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=64, target_modules={'v_proj', 'q_proj'}, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)\n",
      "Module: \n",
      "Module: base_model\n",
      "Module: base_model.model\n",
      "Module: base_model.model.model\n",
      "Module: base_model.model.model.embed_tokens\n",
      "Module: base_model.model.model.layers\n",
      "Module: base_model.model.model.layers.0\n",
      "Module: base_model.model.model.layers.0.self_attn\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.0.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.0.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.0.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.0.mlp\n",
      "Module: base_model.model.model.layers.0.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.0.mlp.up_proj\n",
      "Module: base_model.model.model.layers.0.mlp.down_proj\n",
      "Module: base_model.model.model.layers.0.mlp.act_fn\n",
      "Module: base_model.model.model.layers.0.input_layernorm\n",
      "Module: base_model.model.model.layers.0.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.1\n",
      "Module: base_model.model.model.layers.1.self_attn\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.1.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.1.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.1.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.1.mlp\n",
      "Module: base_model.model.model.layers.1.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.1.mlp.up_proj\n",
      "Module: base_model.model.model.layers.1.mlp.down_proj\n",
      "Module: base_model.model.model.layers.1.mlp.act_fn\n",
      "Module: base_model.model.model.layers.1.input_layernorm\n",
      "Module: base_model.model.model.layers.1.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.2\n",
      "Module: base_model.model.model.layers.2.self_attn\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.2.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.2.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.2.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.2.mlp\n",
      "Module: base_model.model.model.layers.2.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.2.mlp.up_proj\n",
      "Module: base_model.model.model.layers.2.mlp.down_proj\n",
      "Module: base_model.model.model.layers.2.mlp.act_fn\n",
      "Module: base_model.model.model.layers.2.input_layernorm\n",
      "Module: base_model.model.model.layers.2.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.3\n",
      "Module: base_model.model.model.layers.3.self_attn\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.3.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.3.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.3.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.3.mlp\n",
      "Module: base_model.model.model.layers.3.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.3.mlp.up_proj\n",
      "Module: base_model.model.model.layers.3.mlp.down_proj\n",
      "Module: base_model.model.model.layers.3.mlp.act_fn\n",
      "Module: base_model.model.model.layers.3.input_layernorm\n",
      "Module: base_model.model.model.layers.3.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.4\n",
      "Module: base_model.model.model.layers.4.self_attn\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.4.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.4.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.4.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.4.mlp\n",
      "Module: base_model.model.model.layers.4.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.4.mlp.up_proj\n",
      "Module: base_model.model.model.layers.4.mlp.down_proj\n",
      "Module: base_model.model.model.layers.4.mlp.act_fn\n",
      "Module: base_model.model.model.layers.4.input_layernorm\n",
      "Module: base_model.model.model.layers.4.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.5\n",
      "Module: base_model.model.model.layers.5.self_attn\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.5.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.5.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.5.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.5.mlp\n",
      "Module: base_model.model.model.layers.5.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.5.mlp.up_proj\n",
      "Module: base_model.model.model.layers.5.mlp.down_proj\n",
      "Module: base_model.model.model.layers.5.mlp.act_fn\n",
      "Module: base_model.model.model.layers.5.input_layernorm\n",
      "Module: base_model.model.model.layers.5.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.6\n",
      "Module: base_model.model.model.layers.6.self_attn\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.6.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.6.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.6.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.6.mlp\n",
      "Module: base_model.model.model.layers.6.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.6.mlp.up_proj\n",
      "Module: base_model.model.model.layers.6.mlp.down_proj\n",
      "Module: base_model.model.model.layers.6.mlp.act_fn\n",
      "Module: base_model.model.model.layers.6.input_layernorm\n",
      "Module: base_model.model.model.layers.6.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.7\n",
      "Module: base_model.model.model.layers.7.self_attn\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.7.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.7.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.7.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.7.mlp\n",
      "Module: base_model.model.model.layers.7.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.7.mlp.up_proj\n",
      "Module: base_model.model.model.layers.7.mlp.down_proj\n",
      "Module: base_model.model.model.layers.7.mlp.act_fn\n",
      "Module: base_model.model.model.layers.7.input_layernorm\n",
      "Module: base_model.model.model.layers.7.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.8\n",
      "Module: base_model.model.model.layers.8.self_attn\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.8.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.8.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.8.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.8.mlp\n",
      "Module: base_model.model.model.layers.8.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.8.mlp.up_proj\n",
      "Module: base_model.model.model.layers.8.mlp.down_proj\n",
      "Module: base_model.model.model.layers.8.mlp.act_fn\n",
      "Module: base_model.model.model.layers.8.input_layernorm\n",
      "Module: base_model.model.model.layers.8.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.9\n",
      "Module: base_model.model.model.layers.9.self_attn\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.9.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.9.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.9.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.9.mlp\n",
      "Module: base_model.model.model.layers.9.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.9.mlp.up_proj\n",
      "Module: base_model.model.model.layers.9.mlp.down_proj\n",
      "Module: base_model.model.model.layers.9.mlp.act_fn\n",
      "Module: base_model.model.model.layers.9.input_layernorm\n",
      "Module: base_model.model.model.layers.9.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.10\n",
      "Module: base_model.model.model.layers.10.self_attn\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.10.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.10.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.10.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.10.mlp\n",
      "Module: base_model.model.model.layers.10.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.10.mlp.up_proj\n",
      "Module: base_model.model.model.layers.10.mlp.down_proj\n",
      "Module: base_model.model.model.layers.10.mlp.act_fn\n",
      "Module: base_model.model.model.layers.10.input_layernorm\n",
      "Module: base_model.model.model.layers.10.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.11\n",
      "Module: base_model.model.model.layers.11.self_attn\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.11.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.11.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.11.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.11.mlp\n",
      "Module: base_model.model.model.layers.11.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.11.mlp.up_proj\n",
      "Module: base_model.model.model.layers.11.mlp.down_proj\n",
      "Module: base_model.model.model.layers.11.mlp.act_fn\n",
      "Module: base_model.model.model.layers.11.input_layernorm\n",
      "Module: base_model.model.model.layers.11.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.12\n",
      "Module: base_model.model.model.layers.12.self_attn\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.12.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.12.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.12.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.12.mlp\n",
      "Module: base_model.model.model.layers.12.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.12.mlp.up_proj\n",
      "Module: base_model.model.model.layers.12.mlp.down_proj\n",
      "Module: base_model.model.model.layers.12.mlp.act_fn\n",
      "Module: base_model.model.model.layers.12.input_layernorm\n",
      "Module: base_model.model.model.layers.12.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.13\n",
      "Module: base_model.model.model.layers.13.self_attn\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.13.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.13.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.13.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.13.mlp\n",
      "Module: base_model.model.model.layers.13.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.13.mlp.up_proj\n",
      "Module: base_model.model.model.layers.13.mlp.down_proj\n",
      "Module: base_model.model.model.layers.13.mlp.act_fn\n",
      "Module: base_model.model.model.layers.13.input_layernorm\n",
      "Module: base_model.model.model.layers.13.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.14\n",
      "Module: base_model.model.model.layers.14.self_attn\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.14.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.14.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.14.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.14.mlp\n",
      "Module: base_model.model.model.layers.14.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.14.mlp.up_proj\n",
      "Module: base_model.model.model.layers.14.mlp.down_proj\n",
      "Module: base_model.model.model.layers.14.mlp.act_fn\n",
      "Module: base_model.model.model.layers.14.input_layernorm\n",
      "Module: base_model.model.model.layers.14.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.15\n",
      "Module: base_model.model.model.layers.15.self_attn\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.15.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.15.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.15.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.15.mlp\n",
      "Module: base_model.model.model.layers.15.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.15.mlp.up_proj\n",
      "Module: base_model.model.model.layers.15.mlp.down_proj\n",
      "Module: base_model.model.model.layers.15.mlp.act_fn\n",
      "Module: base_model.model.model.layers.15.input_layernorm\n",
      "Module: base_model.model.model.layers.15.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.16\n",
      "Module: base_model.model.model.layers.16.self_attn\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.16.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.16.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.16.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.16.mlp\n",
      "Module: base_model.model.model.layers.16.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.16.mlp.up_proj\n",
      "Module: base_model.model.model.layers.16.mlp.down_proj\n",
      "Module: base_model.model.model.layers.16.mlp.act_fn\n",
      "Module: base_model.model.model.layers.16.input_layernorm\n",
      "Module: base_model.model.model.layers.16.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.17\n",
      "Module: base_model.model.model.layers.17.self_attn\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.17.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.17.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.17.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.17.mlp\n",
      "Module: base_model.model.model.layers.17.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.17.mlp.up_proj\n",
      "Module: base_model.model.model.layers.17.mlp.down_proj\n",
      "Module: base_model.model.model.layers.17.mlp.act_fn\n",
      "Module: base_model.model.model.layers.17.input_layernorm\n",
      "Module: base_model.model.model.layers.17.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.18\n",
      "Module: base_model.model.model.layers.18.self_attn\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.18.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.18.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.18.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.18.mlp\n",
      "Module: base_model.model.model.layers.18.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.18.mlp.up_proj\n",
      "Module: base_model.model.model.layers.18.mlp.down_proj\n",
      "Module: base_model.model.model.layers.18.mlp.act_fn\n",
      "Module: base_model.model.model.layers.18.input_layernorm\n",
      "Module: base_model.model.model.layers.18.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.19\n",
      "Module: base_model.model.model.layers.19.self_attn\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.19.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.19.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.19.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.19.mlp\n",
      "Module: base_model.model.model.layers.19.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.19.mlp.up_proj\n",
      "Module: base_model.model.model.layers.19.mlp.down_proj\n",
      "Module: base_model.model.model.layers.19.mlp.act_fn\n",
      "Module: base_model.model.model.layers.19.input_layernorm\n",
      "Module: base_model.model.model.layers.19.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.20\n",
      "Module: base_model.model.model.layers.20.self_attn\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.20.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.20.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.20.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.20.mlp\n",
      "Module: base_model.model.model.layers.20.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.20.mlp.up_proj\n",
      "Module: base_model.model.model.layers.20.mlp.down_proj\n",
      "Module: base_model.model.model.layers.20.mlp.act_fn\n",
      "Module: base_model.model.model.layers.20.input_layernorm\n",
      "Module: base_model.model.model.layers.20.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.21\n",
      "Module: base_model.model.model.layers.21.self_attn\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.21.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.21.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.21.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.21.mlp\n",
      "Module: base_model.model.model.layers.21.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.21.mlp.up_proj\n",
      "Module: base_model.model.model.layers.21.mlp.down_proj\n",
      "Module: base_model.model.model.layers.21.mlp.act_fn\n",
      "Module: base_model.model.model.layers.21.input_layernorm\n",
      "Module: base_model.model.model.layers.21.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.22\n",
      "Module: base_model.model.model.layers.22.self_attn\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.22.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.22.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.22.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.22.mlp\n",
      "Module: base_model.model.model.layers.22.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.22.mlp.up_proj\n",
      "Module: base_model.model.model.layers.22.mlp.down_proj\n",
      "Module: base_model.model.model.layers.22.mlp.act_fn\n",
      "Module: base_model.model.model.layers.22.input_layernorm\n",
      "Module: base_model.model.model.layers.22.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.23\n",
      "Module: base_model.model.model.layers.23.self_attn\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.23.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.23.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.23.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.23.mlp\n",
      "Module: base_model.model.model.layers.23.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.23.mlp.up_proj\n",
      "Module: base_model.model.model.layers.23.mlp.down_proj\n",
      "Module: base_model.model.model.layers.23.mlp.act_fn\n",
      "Module: base_model.model.model.layers.23.input_layernorm\n",
      "Module: base_model.model.model.layers.23.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.24\n",
      "Module: base_model.model.model.layers.24.self_attn\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.24.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.24.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.24.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.24.mlp\n",
      "Module: base_model.model.model.layers.24.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.24.mlp.up_proj\n",
      "Module: base_model.model.model.layers.24.mlp.down_proj\n",
      "Module: base_model.model.model.layers.24.mlp.act_fn\n",
      "Module: base_model.model.model.layers.24.input_layernorm\n",
      "Module: base_model.model.model.layers.24.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.25\n",
      "Module: base_model.model.model.layers.25.self_attn\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.25.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.25.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.25.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.25.mlp\n",
      "Module: base_model.model.model.layers.25.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.25.mlp.up_proj\n",
      "Module: base_model.model.model.layers.25.mlp.down_proj\n",
      "Module: base_model.model.model.layers.25.mlp.act_fn\n",
      "Module: base_model.model.model.layers.25.input_layernorm\n",
      "Module: base_model.model.model.layers.25.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.26\n",
      "Module: base_model.model.model.layers.26.self_attn\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.26.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.26.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.26.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.26.mlp\n",
      "Module: base_model.model.model.layers.26.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.26.mlp.up_proj\n",
      "Module: base_model.model.model.layers.26.mlp.down_proj\n",
      "Module: base_model.model.model.layers.26.mlp.act_fn\n",
      "Module: base_model.model.model.layers.26.input_layernorm\n",
      "Module: base_model.model.model.layers.26.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.27\n",
      "Module: base_model.model.model.layers.27.self_attn\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.27.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.27.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.27.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.27.mlp\n",
      "Module: base_model.model.model.layers.27.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.27.mlp.up_proj\n",
      "Module: base_model.model.model.layers.27.mlp.down_proj\n",
      "Module: base_model.model.model.layers.27.mlp.act_fn\n",
      "Module: base_model.model.model.layers.27.input_layernorm\n",
      "Module: base_model.model.model.layers.27.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.28\n",
      "Module: base_model.model.model.layers.28.self_attn\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.28.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.28.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.28.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.28.mlp\n",
      "Module: base_model.model.model.layers.28.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.28.mlp.up_proj\n",
      "Module: base_model.model.model.layers.28.mlp.down_proj\n",
      "Module: base_model.model.model.layers.28.mlp.act_fn\n",
      "Module: base_model.model.model.layers.28.input_layernorm\n",
      "Module: base_model.model.model.layers.28.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.29\n",
      "Module: base_model.model.model.layers.29.self_attn\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.29.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.29.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.29.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.29.mlp\n",
      "Module: base_model.model.model.layers.29.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.29.mlp.up_proj\n",
      "Module: base_model.model.model.layers.29.mlp.down_proj\n",
      "Module: base_model.model.model.layers.29.mlp.act_fn\n",
      "Module: base_model.model.model.layers.29.input_layernorm\n",
      "Module: base_model.model.model.layers.29.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.30\n",
      "Module: base_model.model.model.layers.30.self_attn\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.30.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.30.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.30.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.30.mlp\n",
      "Module: base_model.model.model.layers.30.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.30.mlp.up_proj\n",
      "Module: base_model.model.model.layers.30.mlp.down_proj\n",
      "Module: base_model.model.model.layers.30.mlp.act_fn\n",
      "Module: base_model.model.model.layers.30.input_layernorm\n",
      "Module: base_model.model.model.layers.30.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.31\n",
      "Module: base_model.model.model.layers.31.self_attn\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.31.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.31.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.31.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.31.mlp\n",
      "Module: base_model.model.model.layers.31.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.31.mlp.up_proj\n",
      "Module: base_model.model.model.layers.31.mlp.down_proj\n",
      "Module: base_model.model.model.layers.31.mlp.act_fn\n",
      "Module: base_model.model.model.layers.31.input_layernorm\n",
      "Module: base_model.model.model.layers.31.post_attention_layernorm\n",
      "Module: base_model.model.model.norm\n",
      "Module: base_model.model.lm_head\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_embedding_B\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_embedding_B\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "print(peft_config)  # Print your LoRA configuration to confirm it's set up correctly\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    print(f\"Module: {name}\")\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if any(lora_term in name.lower() for lora_term in ['lora', 'adapter', 'peft']):\n",
    "        print(f\"Potential LoRA adapter found in: {name}\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable parameter: {name}\")\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if 'lora' in name.lower():\n",
    "        print(f\"LoRA adapter found in: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e1702",
   "metadata": {},
   "source": [
    "## Setting Up Training Arguments\n",
    "\n",
    "**What it's doing:**\n",
    "Configuring the training process parameters.\n",
    "\n",
    "**Why:**\n",
    "These arguments define crucial aspects of our training process, such as learning rate, batch size, and evaluation frequency. They significantly impact the efficiency and effectiveness of fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40f9860b-354a-41cd-85fe-a884c24200f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "  output_dir = \"mistral_end_to_end\",\n",
    "  #num_train_epochs=5,\n",
    "  max_steps = 100, # comment out this line if you want to train in epochs\n",
    "  per_device_train_batch_size = 4,\n",
    "  warmup_steps = 0,\n",
    "  logging_steps=10,\n",
    "  save_strategy=\"epoch\",\n",
    "  #evaluation_strategy=\"epoch\",\n",
    "  evaluation_strategy=\"steps\",\n",
    "  eval_steps=20, # comment out this line if you want to evaluate at the end of each epoch\n",
    "  learning_rate=2e-4,\n",
    "  bf16=True,\n",
    "  lr_scheduler_type='constant',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9869b6e7",
   "metadata": {},
   "source": [
    "## Setting Up the Trainer\n",
    "\n",
    "**What it's doing:**\n",
    "Initializing the SFTTrainer with our model, datasets, and training configuration.\n",
    "\n",
    "**Why:**\n",
    "The trainer handles the fine-tuning process, managing the training loop, evaluation, and logging. This setup brings together all the components we've prepared for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96495acf-a548-44b2-9e85-2122b8cdf99a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27262976 || all params: 3779334144 || trainable%: 0.7213698223345028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:421: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=model,\n",
    "  peft_config=peft_config,\n",
    "  max_seq_length=max_seq_length,\n",
    "  tokenizer=tokenizer,\n",
    "  packing=True,\n",
    "  formatting_func=create_prompt,\n",
    "  args=args,\n",
    "  train_dataset=instruct_tune_dataset[\"train\"],\n",
    "  eval_dataset=instruct_tune_dataset[\"test\"]\n",
    ")\n",
    "\n",
    "trainable_params = 0\n",
    "all_param = 0\n",
    "for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80708a5",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "**What it's doing:**\n",
    "Running the fine-tuning process and testing the result.\n",
    "\n",
    "**Why:**\n",
    "This is the main training step where our model learns from the prepared dataset. After training, we test it on a sample input to verify improvement and check resource usage to understand the computational cost of our fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d17a6d6-1793-4619-b1f6-ccdf3ccc36ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.543900</td>\n",
       "      <td>1.336502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.411200</td>\n",
       "      <td>1.298247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.427800</td>\n",
       "      <td>1.285116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.414400</td>\n",
       "      <td>1.279086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.339400</td>\n",
       "      <td>1.272258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Input:\n",
      "<s>### Instruction:\n",
      "Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
      "\n",
      "### Input:\n",
      "There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\n",
      "\n",
      "### Response:\n",
      "What are different types of grass?\n",
      "\n",
      "### Response</s>\n",
      "\n",
      "Model Output:\n",
      "<s></s>\n",
      "GPU memory allocated: 4.99 GB\n",
      "GPU memory cached: 22.08 GB\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "sample_input = instruct_tune_dataset[\"train\"][0]\n",
    "formatted_input = create_prompt(sample_input)\n",
    "print(\"Sample Input:\")\n",
    "print(formatted_input)\n",
    "print(\"\\nModel Output:\")\n",
    "print(generate_response(formatted_input, model))\n",
    "\n",
    "import torch\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "print(f\"GPU memory cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1203ffbd-c8c0-46e0-87aa-47daa488ac78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2722580432891846, 'eval_runtime': 3.7054, 'eval_samples_per_second': 4.048, 'eval_steps_per_second': 0.54, 'epoch': 0.8}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc39d7f-4b16-4c4b-8e73-12c83ec04eb4",
   "metadata": {},
   "source": [
    "### let's deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0174e2ff-5073-46b5-907e-c6a09e8621c6",
   "metadata": {},
   "source": [
    "**What it's doing?**\n",
    "This code installs the following Python packages:\n",
    "- `fastapi`: A high-performance web framework for building APIs with Python\n",
    "- `uvicorn`: An ASGI (Asynchronous Server Gateway Interface) web server for running FastAPI applications\n",
    "- `transformers`: A library for state-of-the-art natural language processing (NLP) models\n",
    "- `torch`: The PyTorch machine learning library\n",
    "- `nest_asyncio`: A library that allows nested event loops in Python, which is needed for some asynchronous operations\n",
    "- `requests`: A popular library for making HTTP requests in Python\n",
    "- `torchvision`: A library that provides access to popular datasets, model architectures, and common image transformations for computer vision\n",
    "\n",
    "**Why:**\n",
    "These packages are needed for building a web application that uses machine learning models for natural language processing or computer vision tasks. FastAPI and Uvicorn are used to create the web server and API, while the other packages provide the necessary machine learning functionality and utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca74ba63-b744-48c8-8f6a-850c788d713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Using cached fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn\n",
      "  Using cached uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: transformers in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (4.42.3)\n",
      "Requirement already satisfied: torch in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: nest_asyncio in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: torchvision in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.17.0+cu121)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi)\n",
      "  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from fastapi) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from fastapi) (4.11.0)\n",
      "Collecting fastapi-cli>=0.0.2 (from fastapi)\n",
      "  Using cached fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from fastapi) (3.1.3)\n",
      "Collecting python-multipart>=0.0.7 (from fastapi)\n",
      "  Using cached python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi)\n",
      "  Using cached ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
      "Collecting orjson>=3.2.1 (from fastapi)\n",
      "  Using cached orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Collecting email_validator>=2.0.0 (from fastapi)\n",
      "  Using cached email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: click>=7.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.1.105)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests) (2024.2.2)\n",
      "Collecting torch\n",
      "  Using cached torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torchvision) (10.2.0)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting triton==2.2.0 (from torch)\n",
      "  Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi)\n",
      "  Using cached dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting typer>=0.12.3 (from fastapi-cli>=0.0.2->fastapi)\n",
      "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: anyio in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi) (4.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi) (2.1.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.18.2)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi)\n",
      "  Using cached httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0->fastapi)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi)\n",
      "  Using cached uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi)\n",
      "  Using cached watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0->fastapi)\n",
      "  Using cached websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from anyio->httpx>=0.23.0->fastapi) (1.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (13.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (0.1.2)\n",
      "Using cached fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "Using cached uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
      "Using cached torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "Using cached email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Using cached fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
      "Using cached orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "Using cached python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Using cached starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "Using cached ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
      "Using cached dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "Using cached httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "Using cached uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "Using cached watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Installing collected packages: websockets, uvloop, uvicorn, ujson, triton, shellingham, python-multipart, python-dotenv, orjson, nvidia-nccl-cu12, httptools, dnspython, watchfiles, starlette, email_validator, typer, torch, fastapi-cli, fastapi\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.3.1\n",
      "    Uninstalling triton-2.3.1:\n",
      "      Successfully uninstalled triton-2.3.1\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.9.4\n",
      "    Uninstalling typer-0.9.4:\n",
      "      Successfully uninstalled typer-0.9.4\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.1\n",
      "    Uninstalling torch-2.3.1:\n",
      "      Successfully uninstalled torch-2.3.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
      "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dnspython-2.6.1 email_validator-2.2.0 fastapi-0.111.0 fastapi-cli-0.0.4 httptools-0.6.1 nvidia-nccl-cu12-2.19.3 orjson-3.10.6 python-dotenv-1.0.1 python-multipart-0.0.9 shellingham-1.5.4 starlette-0.37.2 torch-2.2.0 triton-2.2.0 typer-0.12.3 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-12.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastapi uvicorn transformers torch nest_asyncio requests torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43349048-bbdd-41c9-ac27-16398df86e65",
   "metadata": {},
   "source": [
    "**What it's doing?**\n",
    "This code snippet demonstrates how to load the \"Instruct-V3\" dataset using the `load_dataset` function from the `datasets` library. The dataset is loaded from the \"mosaicml/instruct-v3\" dataset identifier, and the first example from the \"train\" split is printed to the console.\n",
    "\n",
    "**Why:**\n",
    "The Instruct-V3 dataset is a popular natural language processing dataset that can be used for training and evaluating language models. By loading the dataset programmatically, you can easily access the data and use it in your own machine learning projects or experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d2990c4-ae35-4b43-81be-e61a38ba7ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction\\nQuestion: Nancy and Rose are making bracelets, and there are eight beads in each bracelet. Nancy has 40 metal beads and 20 more pearl beads. Rose has 20 crystal beads and twice as many stone beads as crystal beads. How many bracelets can Nancy and Rose make?\\nAnswer: Nancy has 40 + 20 = 60 pearl beads. So, Nancy has a total of 40 + 60 = 100 beads. Rose has 2 x 20 = 40 stone beads. So, Rose has 20 + 40 = 60 beads. Thus, Nancy and Rose have 100 + 60 = 160 beads altogether. Therefore, they can make 160 / 8 = 20 bracelets. The answer is 20.\\n[Question]Ms. Estrella is an entrepreneur with a startup company having 10 employees. The company makes a revenue of $400000 a month, paying 10% in taxes, 5% of the remaining amount on marketing and ads, 20% of the remaining amount on operational costs, and 15% of the remaining amount on employee wages. Assuming each employee receives the same wage, calculate the amount of money each employee is paid monthly.\\n[Answer]The company pays a total of 10 / 100 * $400000 = $40000 on taxes. After taxes, the company revenue is $400000 - $40000 = $360,000. The costs of marketing and ads campaign are 5 / 100 * $360000 = $18000. After deducting the costs of marketing and adds campaign, the company remains with = $342,000 in revenue. Operational costs for the company are 20 / 100 * $342000 = $68400. After taking out the operational costs, the company remains with $342000 - $68400 = $273600. The company also pays employee wages of 15 / 100 * $273600 = $41040. If the total number of employees is 10, each employee is paid a salary of $41040 / 10 = $4104 per month. The answer is 4104.\\nQ: Princeton had a gender reveal party and invited all of his fellow employees to celebrate with him. If the total number of guests were 60, and 2/3 were male guests, how many female guests attended the party?\\nA: The number of males in the party was 2 males / 3 guests * 60 guests = 40 males. If the total number of people at the party was 60, then there were 60 guests - 40 males = 20 females. The answer is 20.\\nQuestion: Joshua bought 25 oranges for $12.50. If he sells each one for 60c, how much profit in cents will he make on each orange?\\nAnswer: $1 is equivalent to 100 cents so $12.50 is equivalent to 100 * 12.50 = 1250 cents. He bought 25 oranges for 1250 cents so each orange cost 1250 / 25 = 50 cents each. If he sells each orange for 60 cents, he is making a profit of 60 - 50 = 10 cents on each one. The answer is 10.\\n[Question]Wilson decides to go sledding on some nearby hills. On the 2 tall hills, he sleds down them 4 times each and on the 3 small hills, he sled down them half as often as he sleds down the tall hills. How many times did he sled down the hills?\\n[Answer]On the tall hills, he sleds down 2 tall hills * 4 times per tall hill = 8 times. He sleds down the small hills half as often so he sleds down each hill 4 times per tall hill / 2 = 2 times per small hill. On the small hills, he sleds down 3 small hills * 2 times per small hill = 6 times. So in total, Wilson sled down the hills 8 times on tall hills + 6 times on small hills = 14 times. The answer is 14.\\n[Question]Natalia is riding a bicycle for the cycling competition. On Monday she rode 40 kilometers and on Tuesday 50 kilometers. On Wednesday she rode 50% fewer kilometers than the day before. On Thursday she rode as many as the sum of the kilometers from Monday and Wednesday. How many kilometers did Natalie ride in total?\\n[Answer]\\n### Response\\n', 'response': 'On Wednesday she covered half of the distance from Tuesday, so 50 / 2 = 25 kilometers. On Thursday she rode 40 + 25 = 65 kilometers. In total, Natalia rode 40 + 50 + 25 + 65 = 180 kilometers. The answer is 180.', 'source': 'cot_gsm8k'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mosaicml/instruct-v3\")\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b83e4-0b5f-45f1-ae92-991cb322348c",
   "metadata": {},
   "source": [
    "**What it's doing?**\n",
    "This code snippet imports the `transformers` library, which is a popular library for working with state-of-the-art natural language processing (NLP) models. It then creates a `pipeline` object from the `transformers` library and prints the version number of the `transformers` library.\n",
    "\n",
    "**Why:**\n",
    "The `transformers` library provides a high-level interface for using pre-trained NLP models, such as BERT, GPT, and RoBERTa, for tasks like text classification, named entity recognition, and question answering. By importing and using this library, you can easily leverage these powerful models in your own machine learning projects.\n",
    "\n",
    "Printing the version number can be useful for ensuring that you're using a compatible version of the library with your project or for troubleshooting issues that may be version-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c84155f-f6a9-4832-afcf-5747c7c87118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.42.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e10498-25c9-4283-a4bf-31db3fa477a5",
   "metadata": {},
   "source": [
    "**What it's doing?**\n",
    "This code sets up a FastAPI web application with a text generation endpoint. The application uses the Transformers library to load a fine-tuned language model and generate text based on a provided prompt.\n",
    "\n",
    "The key steps are:\n",
    "\n",
    "1. Import necessary libraries and modules, including `FastAPI`, `Transformers`, and `Torch`.\n",
    "2. Define a `Prompt` model using `pydantic.BaseModel` to represent the input data for the text generation endpoint.\n",
    "3. Implement the `generate_text` function that takes a `Prompt` object, loads the fine-tuned language model, and generates text based on the provided prompt.\n",
    "4. Add a `shutdown` endpoint to the FastAPI app to allow gracefully shutting down the server.\n",
    "5. Define a `run_server` function that starts the Uvicorn server in a separate thread.\n",
    "6. Test the API by sending a POST request to the `/generate` endpoint and print the generated text.\n",
    "7. Implement a `shutdown_server` function to shut down the server.\n",
    "\n",
    "**Why:**\n",
    "This code demonstrates how to build a web application using FastAPI that can generate text using a fine-tuned language model from the Transformers library. This can be useful for various applications, such as:\n",
    "\n",
    "- Building a text generation API for creative writing, translation, or summarization tasks.\n",
    "- Integrating a text generation model into a larger application or system.\n",
    "- Experimenting with fine-tuning language models and deploying the resulting model in a production-ready environment.\n",
    "\n",
    "The use of a separate thread to run the server allows the application to be responsive and handle multiple requests concurrently. The shutdown endpoint provides a way to gracefully terminate the server when needed.\n",
    "\n",
    "This artifact can be used as a starting point for building more complex text generation applications or as a reference for integrating Transformers models into a web-based system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8455cec0-d41d-40e9-a739-e659670782ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [342]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122b261661724dac9dd541015fad0d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Prompt: Instruction: Translate the following English text to French.\n",
      "Input: Hello, how are you?\n",
      "Output:\n",
      "Generated Text: Instruction: Translate the following English text to French.\n",
      "Input: Hello, how are you?\n",
      "Output: Bonjour, comment allez vous?\n",
      "\n",
      "### Response:\n",
      "I need to translate this text from English to French: Hello, how are you?\n",
      "\n",
      "### Response\n",
      "INFO:     127.0.0.1:34000 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "Generated Text: Instruction: Translate the following English text to French.\n",
      "Input: Hello, how are you?\n",
      "Output: Bonjour, comment allez vous?\n",
      "\n",
      "### Response:\n",
      "I need to translate this text from English to French: Hello, how are you?\n",
      "\n",
      "### Response\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# Apply the nest_asyncio patch\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define the Prompt model\n",
    "class Prompt(BaseModel):\n",
    "    instruction: str\n",
    "    input_text: str\n",
    "\n",
    "# Define the text generation endpoint\n",
    "@app.post(\"/generate\")\n",
    "def generate_text(prompt: Prompt):\n",
    "    model_path = \"mistral_end_to_end/checkpoint-100\"  # Path to your fine-tuned model checkpoint\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    nlp = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device)\n",
    "    \n",
    "    # Combine instruction and input_text to form the full prompt\n",
    "    full_prompt = f\"Instruction: {prompt.instruction}\\nInput: {prompt.input_text}\\nOutput:\"\n",
    "    print(f\"Full Prompt: {full_prompt}\")  # Debugging print statement\n",
    "    \n",
    "    outputs = nlp(full_prompt, max_length=100, num_return_sequences=1, truncation=True)\n",
    "    if outputs:\n",
    "        generated_text = outputs[0].get(\"generated_text\", \"\")\n",
    "        print(f\"Generated Text: {generated_text}\")  # Debugging print statement\n",
    "    else:\n",
    "        generated_text = \"\"\n",
    "        print(\"No output generated\")  # Debugging print statement\n",
    "    \n",
    "    return {\"generated_text\": generated_text}\n",
    "\n",
    "# Adding shutdown endpoint to FastAPI app\n",
    "@app.post(\"/shutdown\")\n",
    "def shutdown():\n",
    "    import os\n",
    "    os._exit(0)\n",
    "    return {\"message\": \"Server is shutting down...\"}\n",
    "\n",
    "# Function to run the server\n",
    "def run_server():\n",
    "    import asyncio\n",
    "    asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, loop=\"asyncio\")\n",
    "    server = uvicorn.Server(config)\n",
    "    server.run()\n",
    "\n",
    "# Start the server in a new thread\n",
    "server_thread = Thread(target=run_server)\n",
    "server_thread.start()\n",
    "\n",
    "# Test the API\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Wait a few seconds for the server to start\n",
    "time.sleep(5)\n",
    "\n",
    "# Define the API endpoint\n",
    "api_url = \"http://localhost:8000/generate\"\n",
    "\n",
    "# Define the instruction and input text\n",
    "prompt_data = {\n",
    "    \"instruction\": \"Translate the following English text to French.\",\n",
    "    \"input_text\": \"Hello, how are you?\"\n",
    "}\n",
    "\n",
    "# Send the POST request\n",
    "response = requests.post(api_url, json=prompt_data)\n",
    "\n",
    "# Print the response\n",
    "if response.status_code == 200:\n",
    "    generated_text = response.json().get(\"generated_text\")\n",
    "    print(\"Generated Text:\", generated_text)\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "# Shutdown the server\n",
    "def shutdown_server():\n",
    "    response = requests.post(\"http://localhost:8000/shutdown\")\n",
    "    print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9605ce20-cdf2-4880-a87d-5cde887ca19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the shutdown function\n",
    "shutdown_server()\n",
    "\n",
    "# Wait a bit and then join the thread to ensure it has completed\n",
    "server_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210d1ce-ca1b-4417-a455-00a30e1192dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff01a1-7e36-42b3-9a82-79914021ba42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
